{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching tool "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geinspireerd door : https://towardsdatascience.com/dating-algorithms-using-machine-learning-and-ai-814b68ecd75e \n",
    "data set verkregen door : https://generatedata.com/generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importeren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn\n",
    "# pip install sklearn\n",
    "# pip install tensorflo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data-for_model.json\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorbewerken van de data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaling the data\n",
    "scaler = StandardScaler().fit(df)\n",
    "array_scaled = scaler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(array_scaled, columns=[\n",
    "                         'k1', 'k2', 'k3', 'k4', 'k5', 's1', 's2', 'd4'])\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewijs dat het genormaliseerd :\n",
    "df_scaled.mean()\n",
    "\n",
    "# afgerond is dit 0 --> goed genormaliseerd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster methode bepalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# kiezen van het aantal clusters\n",
    "\n",
    "cluster_cnt = [i for i in range(2, 20, 1)]\n",
    "\n",
    "# Establishing empty lists to store the scores for the evaluation metrics\n",
    "s_scoresA = []\n",
    "db_scoresA = []\n",
    "\n",
    "s_scoresB = []\n",
    "db_scoresB = []\n",
    "\n",
    "# Looping through different iterations for the number of clusters\n",
    "for i in cluster_cnt:\n",
    "\n",
    "    # Hierarchical Agglomerative Clustering with different number of clusters\n",
    "    hac = AgglomerativeClustering(n_clusters=i)\n",
    "\n",
    "    hac.fit(df_scaled)\n",
    "\n",
    "    cluster_assignmentsA = hac.labels_\n",
    "\n",
    "    # KMeans Clustering with different number of clusters\n",
    "    k_means = KMeans(n_clusters=i)\n",
    "\n",
    "    k_means.fit(df_scaled)\n",
    "\n",
    "    cluster_assignmentsB = k_means.predict(df_scaled)\n",
    "\n",
    "    # Appending the scores to the empty lists\n",
    "    s_scoresA.append(silhouette_score(df_scaled, cluster_assignmentsA))\n",
    "    db_scoresA.append(davies_bouldin_score(df_scaled, cluster_assignmentsA))\n",
    "\n",
    "    s_scoresB.append(silhouette_score(df_scaled, cluster_assignmentsB))\n",
    "    db_scoresB.append(davies_bouldin_score(df_scaled, cluster_assignmentsB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation(y, x=cluster_cnt):\n",
    "    \"\"\"\n",
    "    Plots the scores of a set evaluation metric. Prints out the max and min values of the evaluation scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a DataFrame for returning the max and min scores for each cluster\n",
    "    df = pd.DataFrame(columns=['Cluster Score'], index=[\n",
    "                      i for i in range(2, len(y)+2)])\n",
    "    df['Cluster Score'] = y\n",
    "\n",
    "    print('Max Value:\\nCluster #',\n",
    "          df[df['Cluster Score'] == df['Cluster Score'].max()])\n",
    "    print('\\nMin Value:\\nCluster #',\n",
    "          df[df['Cluster Score'] == df['Cluster Score'].min()])\n",
    "\n",
    "    # Plotting out the scores based on cluster count\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Het aantal clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiek 1  - \"The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong/incorrect. \"\n",
    "\n",
    "https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam#:~:text=The%20silhouette%20score%20of%201,value%20of%20the%20K%20(no.\n",
    "\n",
    "\n",
    "Grafiek 2 - \"Davies-Bouldin index is a validation metric that is often used in order to evaluate the optimal number of clusters to use. It is defined as a ratio between the cluster scatter and the cluster's separation and a lower value will mean that the clustering is better \" \n",
    "\n",
    "https://stackoverflow.com/questions/59279056/davies-bouldin-index-higher-or-lower-score-better#:~:text=Davies%2DBouldin%20index%20is%20a,that%20the%20clustering%20is%20better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function on the list of scores\n",
    "plot_evaluation(s_scoresA)\n",
    "plot_evaluation(db_scoresA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus vanuit deze twee grafieken is 16 het optimale aantal clusters. En gebruiken kmeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering\n",
    "\n",
    "grafiek 1 de max is bij 19 clusters met een waarde van 0.129011\n",
    "\n",
    "grafiek 2 de min is bij 19 clusters met een waarde van 1.61575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation(s_scoresB)\n",
    "plot_evaluation(db_scoresB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans \n",
    "grafiek 1 de max is bij 17 clusters met een waarde van 0.140338\n",
    "\n",
    "grafiek 2 de min is bij 19 clusters met een waarde van 1.595732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusie \n",
    "De waarden zijn nagenoeg gelijk voor beide clustering methodes. Maar AgglomerativeClustering heeft een kleine voor sprong. En heeft bij beide evaluatie methodes de piek/dal bij 19 zitten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusteren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating HAC\n",
    "hac = AgglomerativeClustering(n_clusters=19)\n",
    "\n",
    "# Fitting\n",
    "hac.fit(df_scaled)\n",
    "\n",
    "# Getting cluster assignments\n",
    "cluster_assignments = hac.labels_\n",
    "\n",
    "# Assigning the clusters to each profile\n",
    "df_scaled['Cluster #'] = cluster_assignments\n",
    "\n",
    "# Viewing the dating profiles with cluster assignments\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_scaled\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(data['k1'], [data['k2']], c=data[\"Cluster #\"], s=50)\n",
    "ax.set_title(\"Agglomerative Clustering\")\n",
    "ax.set_xlabel(\"K1\")\n",
    "ax.set_ylabel(\"K2\")\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"# Cluster\"] = data[\"Cluster #\"]\n",
    "df_grouped = df.groupby(by=[\"# Cluster\"]).mean()\n",
    "df_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierboven staan alle gemiddelde van de verschillende clusters. Op deze clusters gaat er dus een model gemaakt worden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF -Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"# Cluster\"] = data[\"Cluster #\"]\n",
    "dataset = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geen NaN of gemiste data. --> data klaar voor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitsen in training en test sets\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset, diag_kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De grafiek hierboven laat zien dat er geen duidelijke functies zijn tussen het resultaat (het cluster nummer) en de andere colums. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu verdelen we de dataset in train en test\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('# Cluster')\n",
    "test_labels = test_features.pop('# Cluster')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de verschillende colommen te normaliseren gebruiken we de keras normalisatie laag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))\n",
    "print(normalizer.mean.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = np.array(train_features[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('First example:', first)\n",
    "    print()\n",
    "    print('Normalized:', normalizer(first).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierboven zie je het verschil tussen genormaliseerd en rauwe data. Genormaliseerde data zorgt ervoor dat in het neurale netwerk de gewichten die aan elke berekening/handeling worden gehangen, sneller berekent kunnen worden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model1(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(.2, input_shape=(2,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_and_compile_model1(normalizer)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% % time\n",
    "history = model1.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [Cluster]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_afwijking = model1.evaluate(test_features, test_labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_afwijking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is de gemiddelde absolute afwijking dit berekent het gemiddelde verschil tussen de berekende en de echte waarden. Hoe kleiner deze is, hoe beter het model getraind is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model1.predict(test_features).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('Echte waarden [Clusters]')\n",
    "plt.ylabel('Voorspelling  [Clusters]')\n",
    "lims = [0, 20]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoe dichter de punten bij de rechte lijn liggen. Hoe beter het model voorspelt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interessante layers : \n",
    "* Embedding layer \n",
    "* Dropout layer\n",
    "* Noise layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model2(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(.3, input_shape=(2,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(.3, input_shape=(2,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_and_compile_model2(normalizer)\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(test_features, test_labels, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fb518e8b8505d9187b863b0de8d7abc66f02fc2685ba7cf8ce31cca4e4f69f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
