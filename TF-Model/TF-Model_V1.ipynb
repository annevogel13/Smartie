{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching tool "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geinspireerd door : https://towardsdatascience.com/dating-algorithms-using-machine-learning-and-ai-814b68ecd75e \n",
    "data set verkregen door : https://generatedata.com/generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importeren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn\n",
    "# pip install sklearn\n",
    "# pip install tensorflo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_json(\"data-for_model.json\")  # 500\n",
    "df1 = pd.read_json(\"data-for_model2.json\")  # 500\n",
    "df2 = pd.read_json(\"data-for_model3.json\")  # 500\n",
    "\n",
    "df = pd.concat([df0, df1, df2], ignore_index=True)\n",
    "df  # 1500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorbewerken van de data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaling the data\n",
    "scaler = StandardScaler().fit(df)\n",
    "array_scaled = scaler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(array_scaled, columns=[\n",
    "                         'k1', 'k2', 'k3', 'k4', 'k5', 's1', 's2', 'd4'])\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bewijs dat het genormaliseerd :\n",
    "df_scaled.mean()\n",
    "\n",
    "# afgerond is dit 0 --> goed genormaliseerd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster methode bepalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# kiezen van het aantal clusters\n",
    "\n",
    "cluster_cnt = [i for i in range(2, 40, 1)]\n",
    "\n",
    "# Establishing empty lists to store the scores for the evaluation metrics\n",
    "s_scoresA = []\n",
    "db_scoresA = []\n",
    "\n",
    "s_scoresB = []\n",
    "db_scoresB = []\n",
    "\n",
    "# Looping through different iterations for the number of clusters\n",
    "for i in cluster_cnt:\n",
    "\n",
    "    # Hierarchical Agglomerative Clustering with different number of clusters\n",
    "    hac = AgglomerativeClustering(n_clusters=i)\n",
    "\n",
    "    hac.fit(df_scaled)\n",
    "\n",
    "    cluster_assignmentsA = hac.labels_\n",
    "\n",
    "    # KMeans Clustering with different number of clusters\n",
    "    k_means = KMeans(n_clusters=i)\n",
    "\n",
    "    k_means.fit(df_scaled)\n",
    "\n",
    "    cluster_assignmentsB = k_means.predict(df_scaled)\n",
    "\n",
    "    # Appending the scores to the empty lists\n",
    "    s_scoresA.append(silhouette_score(df_scaled, cluster_assignmentsA))\n",
    "    db_scoresA.append(davies_bouldin_score(df_scaled, cluster_assignmentsA))\n",
    "\n",
    "    s_scoresB.append(silhouette_score(df_scaled, cluster_assignmentsB))\n",
    "    db_scoresB.append(davies_bouldin_score(df_scaled, cluster_assignmentsB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation(y, x=cluster_cnt):\n",
    "    \"\"\"\n",
    "    Plots the scores of a set evaluation metric. Prints out the max and min values of the evaluation scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a DataFrame for returning the max and min scores for each cluster\n",
    "    df = pd.DataFrame(columns=['Cluster Score'], index=[\n",
    "                      i for i in range(2, len(y)+2)])\n",
    "    df['Cluster Score'] = y\n",
    "\n",
    "    print('Max Value:\\nCluster #',\n",
    "          df[df['Cluster Score'] == df['Cluster Score'].max()])\n",
    "    print('\\nMin Value:\\nCluster #',\n",
    "          df[df['Cluster Score'] == df['Cluster Score'].min()])\n",
    "\n",
    "    # Plotting out the scores based on cluster count\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Het aantal clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiek 1  - \"The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong/incorrect. \"\n",
    "\n",
    "https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam#:~:text=The%20silhouette%20score%20of%201,value%20of%20the%20K%20(no.\n",
    "\n",
    "\n",
    "Grafiek 2 - \"Davies-Bouldin index is a validation metric that is often used in order to evaluate the optimal number of clusters to use. It is defined as a ratio between the cluster scatter and the cluster's separation and a lower value will mean that the clustering is better \" \n",
    "\n",
    "https://stackoverflow.com/questions/59279056/davies-bouldin-index-higher-or-lower-score-better#:~:text=Davies%2DBouldin%20index%20is%20a,that%20the%20clustering%20is%20better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function on the list of scores\n",
    "plot_evaluation(s_scoresA)\n",
    "plot_evaluation(db_scoresA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus vanuit deze twee grafieken is 16 het optimale aantal clusters. En gebruiken kmeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering\n",
    "\n",
    "grafiek 1 de max is bij 5 clusters met een waarde van 0.128243\n",
    "\n",
    "grafiek 2 de min is bij 19 clusters met een waarde van 1.61575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation(s_scoresB)\n",
    "plot_evaluation(db_scoresB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans \n",
    "grafiek 1 de max is bij 17 clusters met een waarde van 0.140338\n",
    "\n",
    "grafiek 2 de min is bij 19 clusters met een waarde van 1.595732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusie \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusteren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating HAC\n",
    "\n",
    "NB_CLUSTERS = 5 \n",
    "hac = KMeans(n_clusters= NB_CLUSTERS) # 20 was beste resultaat tot nu toe \n",
    "\n",
    "# AgglomerativeClustering\n",
    "# 5         r2 : 0.818 , RMSE : 0.626 , MEA : 0.43 <-- best \n",
    "# 16        r2 : 0.477 , RMSE : 3.391 , MEA : 2.65 \n",
    "# 25        r2 : 0.488 , RMSE : 5.166 , MEA : 4.12\n",
    "# 30        r2 : 0.416 , RMSE : 6.519 , MEA : 5.05    \n",
    "\n",
    "# KMeans \n",
    "# 5         r2 : 0.957, RMSE : 0.283, MEA : 0.214 <-- best model 2 \n",
    "# 16        r2 : 0.518, RMSE : 3.308, MEA : 2.518 \n",
    "# 25        r2 : 0.387, RMSE : 5.674, MEA : 4.729 \n",
    "# 30        r2 : 0.439, RMSE : 6.400, MEA : 5.089 \n",
    "\n",
    "# Fitting\n",
    "hac.fit(df_scaled)\n",
    "\n",
    "# Getting cluster assignments\n",
    "cluster_assignments = hac.labels_\n",
    "\n",
    "# Assigning the clusters to each profile\n",
    "df_scaled['Cluster #'] = cluster_assignments\n",
    "\n",
    "# Viewing the dating profiles with cluster assignments\n",
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Clusters\"] = df_scaled['Cluster #']\n",
    "data = df\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(data['k1'], [data['k2']], c=data[\"Clusters\"], s=50)\n",
    "ax.set_title(\"Agglomerative Clustering\")\n",
    "ax.set_xlabel(\"K1\")\n",
    "ax.set_ylabel(\"K2\")\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(by=[\"Clusters\"]).mean()\n",
    "df_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierboven staan alle gemiddelde van de verschillende clusters. Op deze clusters gaat er dus een model gemaakt worden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF -Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()\n",
    "dataset.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geen NaN of gemiste data. --> data klaar voor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitsen in training en test sets\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset, diag_kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De grafiek hierboven laat zien dat er geen duidelijke functies zijn tussen het resultaat (het cluster nummer) en de andere colums. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu verdelen we de dataset in train en test\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('Clusters')\n",
    "test_labels = test_features.pop('Clusters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de verschillende colommen te normaliseren gebruiken we de keras normalisatie laag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))\n",
    "print(normalizer.mean.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierboven zie je het verschil tussen genormaliseerd en rauwe data. Genormaliseerde data zorgt ervoor dat in het neurale netwerk de gewichten die aan elke berekening/handeling worden gehangen, sneller berekent kunnen worden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from math import sqrt \n",
    "\n",
    "def evaluation(test, predict):\n",
    "\n",
    "    a = plt.axes(aspect='equal')\n",
    "    plt.scatter(test, predict)\n",
    "    plt.xlabel('Echte waarden [Clusters]')\n",
    "    plt.ylabel('Voorspelling  [Clusters]')\n",
    "    lims = [0, NB_CLUSTERS]  # clusters lopen van 0 tot 19\n",
    "    plt.xlim(lims)\n",
    "    plt.ylim(lims)\n",
    "    _ = plt.plot(lims, lims)\n",
    "\n",
    "    # r2-score is altijd tussen 0 en 1 --> hoe hoger hoe beter\n",
    "    x = r2_score(test, predict)\n",
    "    # mean squared error --> hoe dichter bij nul hoe beter\n",
    "    # tussen 0.2 en 0.5 --> goed model \n",
    "    y = mean_squared_error(test, predict)\n",
    "    print(\"0 < r2 score < 1, best 1                     \", x)\n",
    "    print(\"root mean square error (RMSE), best 0        \", sqrt(y))\n",
    "    print(\"mea, best 0                                  \", mae(test, predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model1(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        #layers.Dense(8, activation='relu'),\n",
    "        layers.Dropout(.2, input_shape=(2,)),\n",
    "        layers.Dense(8, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  metrics=[\"accuracy\"],\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_and_compile_model1(normalizer)\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 2])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.plot(history.history['accuracy'], label='accc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(test_features, test_labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is de gemiddelde absolute afwijking dit berekent het gemiddelde verschil tussen de berekende en de echte waarden. Hoe kleiner deze is, hoe beter het model getraind is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model1.predict(test_features).flatten()\n",
    "\n",
    "evaluation(test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoe dichter de punten bij de rechte lijn liggen. Hoe beter het model voorspelt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interessante layers : \n",
    "* Embedding layer \n",
    "* Dropout layer\n",
    "* Noise layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_and_compile_model2(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(.3, input_shape=(8,)),\n",
    "        layers.GaussianNoise(stddev = 0.2, seed=None),\n",
    "        # layers.Dense(8, activation='relu'),\n",
    "        # layers.Dropout(.3, input_shape=(8,)),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  metrics=[\"accuracy\"],\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_and_compile_model2(normalizer)\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(history2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(test_features, test_labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions2 = model2.predict(test_features).flatten()\n",
    "\n",
    "evaluation(test_labels, test_predictions2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model3(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(.3, input_shape=(8,)),\n",
    "        layers.GaussianNoise(stddev = 0.2, seed=None),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  metrics=[\"accuracy\"],\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_and_compile_model3(normalizer)\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=200)\n",
    "\n",
    "plot_loss(history3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.evaluate(test_features, test_labels, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions3 = model3.predict(test_features)\n",
    "\n",
    "evaluation(test_labels, test_predictions3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the chosen model fits worse than a horizontal line, then R2 is negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fb518e8b8505d9187b863b0de8d7abc66f02fc2685ba7cf8ce31cca4e4f69f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
